{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ahmetsenturk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ahmetsenturk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import json\n",
    "import pickle5 as pickle\n",
    "import nltk\n",
    "import scispacy\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_file = open(\"topics.pickle\", \"rb\")\n",
    "all_documents_file = open(\"all_documents.pickle\", \"rb\")\n",
    "nlp = spacy.load(\"en_core_sci_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_dict = pickle.load(topics_file)\n",
    "all_docs_dict = pickle.load(all_documents_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame(topics_dict)\n",
    "all_docs_df = pd.DataFrame(all_docs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs_df.drop_duplicates(subset=['document_id'], keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, topic in topics_df.iterrows():\n",
    "    nlp_query = nlp(topic[\"query\"]).ents\n",
    "    nlp_question = nlp(topic[\"question\"]).ents\n",
    "    nlp_narrative = nlp(topic[\"narrative\"]).ents\n",
    "    \n",
    "    resulting_query = \"\"\n",
    "    resulting_question = \"\"\n",
    "    resulting_narrative = \"\"\n",
    "    \n",
    "    for query_entites in nlp_query:  \n",
    "        resulting_query = resulting_query + \" \" + str(query_entites).replace(\"\\n\", \" \")\n",
    "        \n",
    "    for question_entites in nlp_question:\n",
    "        resulting_question = resulting_question + \" \" + str(question_entites).replace(\"\\n\", \" \")\n",
    "       \n",
    "    for narrative_entites in nlp_narrative:\n",
    "        resulting_narrative = resulting_narrative + \" \" + str(narrative_entites).replace(\"\\n\", \" \")\n",
    "        \n",
    "    topics_df.iloc[index][\"query\"] = topics_df.iloc[index][\"query\"] + \" \" + resulting_query\n",
    "    topics_df.iloc[index][\"question\"] = topics_df.iloc[index][\"question\"] + \" \" + resulting_question\n",
    "    topics_df.iloc[index][\"narrative\"] = topics_df.iloc[index][\"narrative\"] + \" \" + resulting_narrative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, document in all_docs_df.iterrows():\n",
    "    nlp_abstract = nlp(document[\"abstract\"]).ents\n",
    "    nlp_title = nlp(document[\"title\"]).ents\n",
    "    \n",
    "    resulting_abstract = \"\"\n",
    "    resulting_title = \"\"\n",
    "    \n",
    "    for abstract_entites in nlp_query:  \n",
    "        resulting_abstract = resulting_abstract + \" \" + str(abstract_entites).replace(\"\\n\", \" \")\n",
    "        \n",
    "    for title_entites in nlp_question:\n",
    "        resulting_title = resulting_title + \" \" + str(title_entites).replace(\"\\n\", \" \")\n",
    "         \n",
    "    all_docs_df.iloc[index][\"abstract\"] = all_docs_df.iloc[index][\"abstract\"] + \" \" + resulting_abstract\n",
    "    all_docs_df.iloc[index][\"title\"] = all_docs_df.iloc[index][\"title\"] + \" \" + resulting_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs_df[\"text\"] = all_docs_df[\"abstract\"] + \" \" + all_docs_df[\"title\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "all_docs_splitted = []\n",
    "for text in all_docs_df[\"text\"]:\n",
    "    if text != \"\":\n",
    "        filtered_text = []\n",
    "        for t in tokenizer.tokenize(text):\n",
    "            if t not in stop_words:\n",
    "                filtered_text.append(t.lower())\n",
    "        all_docs_splitted.append(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(all_docs_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "train_results = pd.DataFrame()\n",
    "test_results = pd.DataFrame()\n",
    "for topic_index in range(0, len(topics_df.index)):\n",
    "    query = topics_df.iloc[topic_index][\"query\"] + \" \" + topics_df.iloc[topic_index][\"question\"] + \" \" + topics_df.iloc[topic_index][\"narrative\"]\n",
    "    filtered_query = []\n",
    "    if query != \"\":\n",
    "        for t in tokenizer.tokenize(query):\n",
    "            if t not in stop_words:\n",
    "                filtered_query.append(t.lower())\n",
    "    \n",
    "    query_results = bm25.get_scores(filtered_query)\n",
    "    query_results_df = pd.DataFrame(query_results)\n",
    "    query_results_df.columns = [\"bm25 results\"]\n",
    "    query_results_df.insert(0, \"topic_id\", [(topic_index + 1) for x in range(0, len(query_results_df.index))])\n",
    "    query_results_df.insert(1, \"temp1\", [\"Q0\" for x in range(0, len(query_results_df.index))])\n",
    "    query_results_df.insert(2, \"doc_id\", [all_docs_df.iloc[i,0] for i in range(0, len(query_results_df.index))])\n",
    "    query_results_df.insert(3, \"rank\", [\"0\" for x in range(0, len(query_results_df.index))])\n",
    "    query_results_df[\"standard\"] = \"STANDARD\"\n",
    "    results = pd.concat([results, query_results_df], axis=0) \n",
    "    if topic_index % 2 == 0:\n",
    "        test_results = pd.concat([train_results, query_results_df], axis=0) \n",
    "    else:\n",
    "        train_results = pd.concat([test_results, query_results_df], axis=0)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./results.txt\", results.values, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./test_results.txt\", test_results.values, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./train_results.txt\", train_results.values, fmt=\"%s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
