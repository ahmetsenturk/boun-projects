{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ahmetsenturk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ahmetsenturk/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import json\n",
    "import pickle5 as pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_file = open(\"topics.pickle\", \"rb\")\n",
    "all_documents_file = open(\"all_documents.pickle\", \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_dict = pickle.load(topics_file)\n",
    "all_docs_dict = pickle.load(all_documents_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame(topics_dict)\n",
    "all_docs_df = pd.DataFrame(all_docs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs_df.drop_duplicates(subset=['document_id'], keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs_df[\"text\"] = all_docs_df[\"abstract\"] + \" \" + all_docs_df[\"title\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "all_docs_splitted = []\n",
    "for text in all_docs_df[\"text\"]:\n",
    "    if text != \"\":\n",
    "        filtered_text = []\n",
    "        for t in tokenizer.tokenize(text):\n",
    "            if t not in stop_words:\n",
    "                filtered_text.append(t.lower())\n",
    "        all_docs_splitted.append(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(all_docs_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "train_results = pd.DataFrame()\n",
    "test_results = pd.DataFrame()\n",
    "for topic_index in range(0, len(topics_df.index)):\n",
    "    query = topics_df.iloc[topic_index][\"query\"] + \" \" + topics_df.iloc[topic_index][\"question\"] + \" \" + topics_df.iloc[topic_index][\"narrative\"]\n",
    "    filtered_query = []\n",
    "    if query != \"\":\n",
    "        for t in tokenizer.tokenize(query):\n",
    "            if t not in stop_words:\n",
    "                filtered_query.append(t.lower())\n",
    "    \n",
    "    query_results = bm25.get_scores(filtered_query)\n",
    "    query_results_df = pd.DataFrame(query_results)\n",
    "    query_results_df.columns = [\"bm25 results\"]\n",
    "    query_results_df.insert(0, \"topic_id\", [(topic_index + 1) for x in range(0, len(query_results_df.index))])\n",
    "    query_results_df.insert(1, \"temp1\", [\"Q0\" for x in range(0, len(query_results_df.index))])\n",
    "    query_results_df.insert(2, \"doc_id\", [all_docs_df.iloc[i,0] for i in range(0, len(query_results_df.index))])\n",
    "    query_results_df.insert(3, \"rank\", [\"0\" for x in range(0, len(query_results_df.index))])\n",
    "    query_results_df[\"standard\"] = \"STANDARD\"\n",
    "    results = pd.concat([results, query_results_df], axis=0) \n",
    "    if topic_index % 2 == 0:\n",
    "        test_results = pd.concat([train_results, query_results_df], axis=0) \n",
    "    else:\n",
    "        train_results = pd.concat([test_results, query_results_df], axis=0)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./results.txt\", results.values, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./test_results.txt\", test_results.values, fmt=\"%s\")\n",
    "np.savetxt(\"./train_results.txt\", train_results.values, fmt=\"%s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
